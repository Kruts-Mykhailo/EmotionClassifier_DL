{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configurations and libraries\n",
    "\n",
    "**Important note**\n",
    "\n",
    "Generate the `FER2013+` dataset before running the application\n"
   ],
   "id": "80af76f6f051d33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:10.241513Z",
     "start_time": "2024-12-30T20:41:10.237146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from  torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets.FERPlusDataset import FERPlusDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "\n",
    "# device = \"cpu\"\n",
    "# Paths to dataset directories\n",
    "train_dir = \"./data/FER2013Train\"\n",
    "val_dir = \"./data/FER2013Valid\"\n",
    "test_dir = \"./data/FER2013Test\"\n",
    "\n",
    "# Paths to labels\n",
    "train_csv = \"./data/FER2013Train/label.csv\"\n",
    "val_csv = \"./data/FER2013Valid/label.csv\"\n",
    "test_csv = \"./data/FER2013Test/label.csv\""
   ],
   "id": "bc085bb4922f0f51",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:12.988030Z",
     "start_time": "2024-12-30T20:41:10.276353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_data_availability(image_dir, label_csv):\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"Error: Directory {image_dir} does not exist.\")\n",
    "        return False\n",
    "\n",
    "    png_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "    if len(png_files) == 0:\n",
    "        print(f\"Error: No .png files found in directory {image_dir}.\")\n",
    "        return False\n",
    "\n",
    "    # Check if label CSV exists\n",
    "    if not os.path.exists(label_csv):\n",
    "        print(f\"Error: Label file {label_csv} does not exist.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Data check passed for {image_dir} and {label_csv}.\")\n",
    "    return True\n",
    "\n",
    "# Check all directories\n",
    "if (\n",
    "    check_data_availability(train_dir, train_csv) and\n",
    "    check_data_availability(val_dir, val_csv) and\n",
    "    check_data_availability(test_dir, test_csv)\n",
    "):\n",
    "    print(\"All data is available and ready for training!\")\n",
    "else:\n",
    "    print(\"Data check failed. Please fix the errors and try again.\")\n"
   ],
   "id": "25a65d825f46e3ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check passed for ./data/FER2013Train and ./data/FER2013Train/label.csv.\n",
      "Data check passed for ./data/FER2013Valid and ./data/FER2013Valid/label.csv.\n",
      "Data check passed for ./data/FER2013Test and ./data/FER2013Test/label.csv.\n",
      "All data is available and ready for training!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "`Challenge:`\n",
    "\n",
    "What is the proper way to augment dataset?\n",
    "\n",
    "There are 2 ways to apply augmentation:\n",
    "* On the fly (when loading custom `Dataset`, pass original image and augmented image)\n",
    "* Create a separate augmented dataset and combine with original \n",
    "\n",
    "`Solution:`\n",
    "\n",
    "We chose to create a separate dataset and combine it with original one.\n",
    "\n",
    "\n"
   ],
   "id": "f0053de6e7c74210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:13.043643Z",
     "start_time": "2024-12-30T20:41:12.991036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply transformations and augmentation\n",
    "main_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor() ,  \n",
    "    transforms.Normalize((0.5,), (0.5,), (0.5,),)  \n",
    "])\n",
    "\n",
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),  # Reduce rotation degrees\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05),  # Subtle jitter\n",
    "    transforms.RandomResizedCrop(size=(40, 40), scale=(0.9, 1.0)),  # Reduce crop range\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "train_first_dataset = FERPlusDataset(image_dir=train_dir, aug_transform=train_augmentation, transform=main_transform, label_csv=train_csv, mode=\"majority\")\n",
    "\n",
    "test_dataset = FERPlusDataset(image_dir=test_dir, aug_transform=None, transform=main_transform, label_csv=test_csv, mode=\"majority\")\n",
    "validation_dataset = FERPlusDataset(image_dir=val_dir, aug_transform=None, transform=main_transform, label_csv=val_csv, mode=\"majority\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  \n",
    "    \n",
    "train_loader = DataLoader(train_first_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "604f891ba135babb",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:13.329067Z",
     "start_time": "2024-12-30T20:41:13.072797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Debugging a sample\n",
    "for img, label in train_loader:\n",
    "    print(f\"Image type: {type(img)}, Image shape: {img.shape}\")\n",
    "    break\n"
   ],
   "id": "595582df2b090930",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image type: <class 'torch.Tensor'>, Image shape: torch.Size([64, 3, 48, 48])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Enhancements\n",
    "## Transfer Learning\n",
    " Integrate Transfer Learning for Emotion Classification. In our case we will use ResNet model."
   ],
   "id": "ff22511bacf53a37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Import and Modify Pre-Trained Model",
   "id": "c4853dc82e31949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:49:30.337610Z",
     "start_time": "2024-12-30T20:49:30.003838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load Pre-Trained Model (ResNet50 in this example)\n",
    "pretrained_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the weights of the pre-trained layers to retain learned features.\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final classification layer to suit our classification needs\n",
    "num_classes = 8\n",
    "pretrained_model.fc = nn.Sequential(\n",
    "            nn.Linear(pretrained_model.fc.in_features, 512),\n",
    "            # nn.AdaptiveAvgPool2d(1),  # Global Average Pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)  # Output layer for num_emotions classes\n",
    "        )\n"
   ],
   "id": "19ad98629705d060",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train modified model",
   "id": "cb9340173ab2c544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:13.784363Z",
     "start_time": "2024-12-30T20:41:13.780454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience # Number of epochs to wait before stopping\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ],
   "id": "2e79946a50691e86",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:41:13.803760Z",
     "start_time": "2024-12-30T20:41:13.789372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def early_stopping_triggered(early_stopping, val_loss):\n",
    "    \"\"\"Check and handle early stopping condition.\"\"\"\n",
    "    early_stopping(val_loss)\n",
    "    return early_stopping.early_stop\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train the model for one epoch and compute metrics.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for metrics\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    train_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    train_recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    train_f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return running_loss / len(train_loader), train_accuracy, train_precision, train_recall, train_f1\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the validation set and compute metrics.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # Store predictions and labels for metrics\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    val_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    val_recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    val_f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return val_loss / len(val_loader), val_accuracy, val_precision, val_recall, val_f1\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, early_stopping):\n",
    "    \"\"\"Train the model with validation, metrics, and early stopping.\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(f\"runs/model_{timestamp}\")\n",
    "    training_dir = f\"{checkpoint_dir}/training/\"\n",
    "    os.makedirs(training_dir, exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_accuracy, train_precision, train_recall, train_f1 = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1 = validate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "\n",
    "        if epoch == num_epochs - 1:\n",
    "            print(\"================================\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Train Precision {train_precision:.4f}\")\n",
    "            print(f\"Train Recall: {train_recall:.4f}\")\n",
    "            print(f\"Train F1 score: {train_f1:.4f}\")\n",
    "            print(\"================================\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Val Precision {val_precision:.4f}\")\n",
    "            print(f\"Val Recall: {val_recall:.4f}\")\n",
    "            print(f\"Val F1 score: {val_f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch + 1)\n",
    "        writer.add_scalar(\"Precision/Train\", train_precision, epoch + 1)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = os.path.join(f\"{checkpoint_dir}/training/\", f\"epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Early Stopping\n",
    "        if early_stopping_triggered(early_stopping, val_loss):\n",
    "            print(\"Early stopping triggered!\")\n",
    "            print(\"================================\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "            print(f\"Train Precision {train_precision:.4f}\")\n",
    "            print(f\"Train Recall: {train_recall:.4f}\")\n",
    "            print(f\"Train F1 score: {train_f1:.4f}\")\n",
    "            print(\"================================\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Val Precision {val_precision:.4f}\")\n",
    "            print(f\"Val Recall: {val_recall:.4f}\")\n",
    "            print(f\"Val F1 score: {val_f1:.4f}\")\n",
    "            break"
   ],
   "id": "d8888f8c8fce4d62",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T20:49:34.491116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_stopping = EarlyStopping(patience=8)\n",
    "model = pretrained_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-7)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    early_stopping=early_stopping\n",
    ")"
   ],
   "id": "24454acd962ff611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate the model",
   "id": "1a25105776707da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 score: {f1:.4f}\")\n",
    "    return metrics"
   ],
   "id": "d6bad0b084cdacb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "metrics = evaluate_model(model, test_loader, device)",
   "id": "f420121a01288b91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualise model evaluation",
   "id": "adfe853c3a084346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "emotion_labels = [\n",
    "    \"Neutral\", \"Happiness\", \"Surprise\", \"Sadness\",\n",
    "    \"Anger\", \"Disgust\", \"Fear\", \"Contempt\"\n",
    "]\n",
    "\n",
    "all_labels = []  # True labels\n",
    "all_predictions = []  # Model predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ],
   "id": "7c929c512acb9155"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ensemble Methods\n",
    "Combine predictions from multiple CNN models to create a robust ensemble model"
   ],
   "id": "da4a928157b9d64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensemble_predict(models, dataloader, device):\n",
    "    \"\"\"\n",
    "    Combine predictions from multiple models using an ensemble approach.\n",
    "\n",
    "    Parameters:\n",
    "    - models: List of trained models.\n",
    "    - dataloader: DataLoader for validation or test data.\n",
    "    - device: Device (CPU or GPU) for computation.\n",
    "\n",
    "    Returns:\n",
    "    - Final ensemble predictions.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Store predictions from all models\n",
    "        model_outputs = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                probs = nn.functional.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "                model_outputs.append(probs.cpu().numpy())\n",
    "\n",
    "        # Average probabilities across models\n",
    "        averaged_probs = np.mean(model_outputs, axis=0)  # Shape: [batch_size, num_classes]\n",
    "        final_predictions = np.argmax(averaged_probs, axis=1)  # Final predicted classes\n",
    "        all_predictions.extend(final_predictions)\n",
    "\n",
    "    return np.array(all_predictions)"
   ],
   "id": "e8787fd9fb8bbd34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "custom_model = torch.load(\"models/model_80.pth\")\n",
    "resnet50_model = model\n",
    "\n",
    "ensemble_models = [resnet50_model, custom_model]\n",
    "\n",
    "# Example: Evaluate on test set\n",
    "ensemble_predictions = ensemble_predict(ensemble_models, test_loader, device)\n",
    "print(\"Ensemble Predictions:\", )\n",
    "ensemble_predictions"
   ],
   "id": "a7365587ac2c9922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bcb169fde8b4329"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
